{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Mounting google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8CVII-SoQF8O","executionInfo":{"status":"ok","timestamp":1677866715217,"user_tz":0,"elapsed":1931,"user":{"displayName":"patrik kovac","userId":"01845597780432304560"}},"outputId":"8ae4db77-8910-4216-ad8a-0dec9a6bc067"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBCZXWxmljDk","executionInfo":{"status":"error","timestamp":1677866720360,"user_tz":0,"elapsed":2905,"user":{"displayName":"patrik kovac","userId":"01845597780432304560"}},"colab":{"base_uri":"https://localhost:8080/","height":258},"outputId":"be108544-f5fa-47a5-e3d2-8f882e0eff3a"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a1fd5311c5c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mHIV_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcompanycsv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPROCESSED_DATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcompanycsv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"HIV.csv\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     HIV_dict[companycsv[:-8]] = pd.read_csv(PROCESSED_DATA + companycsv, \n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Text Mining Project IV Prediction/Data (Raw and processed data from python scripts)/Processed Data/'"]}],"source":["# Remember to \"mount drive\" first :\n","# Click on the folder icon on the left, and the third icon from the left is the \n","# mount drive function.\n","\n","import pandas as pd\n","import datetime as dt\n","import numpy as np\n","import os\n","\n","from sklearn.linear_model import Lasso\n","# from sklearn.feature_extraction.text import tfidfvectorizer\n","from sklearn.model_selection import train_test_split\n","\n","# Defining directories\n","PATH_DATA      = \"/content/drive/MyDrive/Text Mining Project IV Prediction/Data (Raw and processed data from python scripts)/\"\n","RAW_DATA       = PATH_DATA + \"Raw Data/\"\n","PROCESSED_DATA = PATH_DATA + \"Processed Data/\"\n","COMPLETED_DATA = PATH_DATA + \"Completed Data/\"\n","\n","# Importing data sets into dictionary\n","tweets_dict = {}\n","HIV_dict = {}\n","\n","for companycsv in os.listdir(PROCESSED_DATA):\n","  if companycsv[-7:] == \"HIV.csv\":\n","    HIV_dict[companycsv[:-8]] = pd.read_csv(PROCESSED_DATA + companycsv, \n","                                            index_col = 0).copy()\n","\n","    \n","  else:\n","    tweets_dict[companycsv[:-11]] = pd.read_csv(PROCESSED_DATA + companycsv, \n","                                                index_col = 0).copy()"]},{"cell_type":"code","source":["COMPLETED_DATA"],"metadata":{"id":"tZ5gDeYTq4UZ","executionInfo":{"status":"error","timestamp":1677867070482,"user_tz":0,"elapsed":8,"user":{"displayName":"Bruce Lau","userId":"03714986190475957458"}},"outputId":"ed9b4a7a-1e60-4d41-be8f-f1664f2533ab","colab":{"base_uri":"https://localhost:8080/","height":169}},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c6fd02bd2dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCOMPLETED_DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'COMPLETED_DATA' is not defined"]}]},{"cell_type":"code","source":["# ============================= CREATING OBJECT ============================== #\n","# ============================================================================ #\n","class DATA:\n","    # tweets and HIV are dictionaries\n","    def __init__(self, tweets, HIV):\n","\n","      # Removing time element in tweets only keeping dates\n","      for company in tweets:\n","        tweets[company][\"post_date\"] = tweets[company][\"post_date\"].apply(\n","            lambda x: x[:-9])\n","      \n","      # Removing data of year 2020 from HIV\n","      for company in HIV:\n","        HIV[company] = HIV[company][HIV[company].copy()[\"Date\"].apply(\n","            lambda x: x[:4] != \"2020\")]\n","      # Setting default object features\n","\n","      self.tweets = tweets\n","      self.HIV = HIV\n","\n","    # =========================== Getting self df ============================ #\n","    # ======================================================================== #\n","\n","    # Getting access to features\n","    def get_tweets(self, company):\n","      try:\n","        return self.tweets[company].copy()\n","      except:\n","        print(\"Company not found\")\n","\n","    def get_HIV(self, company):\n","      try:\n","        return self.HIV[company].copy()\n","      except:\n","        print(\"Company not found\")\n","\n","    def get_companies(self):\n","      if len(self.tweets.keys()) < len(self.HIV.keys()):\n","        return list(self.tweets.keys())\n","      else:\n","        return list(self.HIV.keys())\n","\n","    # ========================= Getting processed df ========================= #\n","    # ======================================================================== #\n","    def get_tweets_per_day(self, company):\n","      df = self.tweets[\"AAPL\"].copy()\n","\n","      tweets_as_series = df.groupby(\"post_date\")[\"body\"].apply(set)\n","      return pd.DataFrame(tweets_as_series)\n","\n","    def get_rolling_tweets(self, company, n_days, n_weeks, start_day = dt.date(\n","        year = 2015, month = 1, day = 9)):\n","      \n","      df = self.get_tweets(company).copy()\n","      df[\"post_date\"] = df[\"post_date\"].apply(pd.to_datetime)\n","\n","      dates = [start_day + i*dt.timedelta(days = n_days) for i in range(n_weeks+1)]\n","      dates = pd.DataFrame(dates, columns = [\"Dates\"]).apply(pd.to_datetime)\n","\n","\n","      tweets_per_week = []\n","      for idx in range(0, len(dates)-1):\n","        # start date of the week and end date of the week\n","        start = dates.iloc[idx][\"Dates\"]\n","        end   = dates.iloc[idx+1][\"Dates\"]\n","\n","        # Filtering df\n","        temp = df[df[\"post_date\"] >= start] # all posts after start date\n","        temp = temp[temp[\"post_date\"] < end] # all posts before end date\n","        \n","        # Creating sets to remove duplicates\n","        tweets = set(temp[\"body\"])\n","\n","        tweets_per_week.append([dates.iloc[idx][\"Dates\"], tweets])\n","\n","      return pd.DataFrame(tweets_per_week, columns = [\"Date\", \"Tweets\"])\n","\n","\n","    def get_HIV_change(self, company, n, cat = \"Implied Vol\"):\n","      df = self.get_HIV(company).copy()\n","      rolling = df.rolling(n)[cat]\n","\n","      df[f\"{cat} change\"] = rolling.apply(lambda x: x.iloc[-1]- x.iloc[0])\n","      \n","      return df\n","\n","    def get_HIV_change_threshold(self, company, n, percentile, cat = \"Implied Vol\"):\n","      change = self.get_HIV_change(company, n, cat)[f\"{cat} change\"].iloc[1:]\n","      return np.percentile(change.apply(abs), percentile)\n","    \n","    def apply_change_threshold(self, company, n, threshold, cat = \"Implied Vol\"):\n","      df = self.get_HIV_change(company, n, cat).copy()\n","      df[\"Changed\"] = df[f\"{cat} change\"].apply(abs) > threshold\n","      \n","      return df\n","\n","\n","    def get_supervised_data(self, comapny, HIV_threshold, tweets_n_days, \n","                            n_weeks, ):\n","      \n","      \n","      HIV = self.apply_change_threshold(company, n_weeks, HIV_threshold).copy()\n","      rolling_tweets = self.get_rolling_tweets(company, n_days = tweets_n_days, \n","                                               n_weeks = 260).copy()\n","\n","      ML_df = rolling_tweets[\"Changed\"] = list(HIV[\"Changed\"].copy())\n","      \n","      return ML_df"],"metadata":{"id":"iAu2-E0Rlo7m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================ Defining Functions ============================ #\n","# ============================================================================ #\n","def ngrams(string, n):\n","  splitted = string.split()\n","  \n","  res = [ splitted[i : i+n] for i in range(len(splitted) - n + 1) ]\n","\n","  return res\n","\n","def vecotriser(lizt):\n","  vec = tfidfvectorizer()\n","  res = []\n","  for ngram in lizt:\n","    try: \n","      res.append(vec.fit_transform(ngram))\n","    except:\n","      continue\n","\n","# transforming 2d list into 1d\n","def all_in_one(lizt):\n","  # print(len(lizt))\n","  res = []\n","  for i in lizt:\n","    res = res+i\n","  return res\n","\n"],"metadata":{"id":"xwNQqQcU8jZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================= SCRAP CODE CHUNK ============================= #\n","# ============================================================================ #\n"],"metadata":{"id":"0HcQl7ZSQj1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================ Setting parameters ============================ #\n","# ============================================================================ #\n","\n","company              = \"MSFT\"\n","threshold_percentile = 50 # this term will determine how many percent of weeks\n","                          # showed change in IV\n","delta_weeks          = 1\n","\n","# ============================= Creating Object ============================== #\n","# ============================================================================ #\n","\n","# Create object\n","main = DATA(tweets_dict, HIV_dict)\n","\n","# Calculating threshold (for considering there is a change)\n","threshold = main.get_HIV_change_threshold(company, delta_weeks+1, \n","                                          threshold_percentile)\n","\n","# Receiving df for boolean change in IV for given threshold\n","HIV = main.apply_change_threshold(company, 2, threshold)\n","\n","# Receiving df for rolling tweets\n","rolling_tweets = main.get_rolling_tweets(company, n_days = 7, n_weeks = 260)\n","\n","# Creating a supervised dataset\n","ML_df = rolling_tweets.copy()\n","ML_df[\"Changed\"] = list(HIV[\"Changed\"].copy())\n"],"metadata":{"id":"UDPm5rOkqnb2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== Data Cleansing ============================== #\n","# ============================================================================ #\n","\n","ML_df[\"Tweets no link\"] = ML_df[\"Tweets\"].apply(\n","    lambda x: [i for i in x if \"http\" not in i])\n","\n","ML_df.to_csv(f\"{COMPLETED_DATA}MSFT_delta1week_v1.csv\")"],"metadata":{"id":"k2TNH4p9H5rb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================ Data Restructuring ============================ #\n","# ============================================================================ #\n","\n","res = pd.DataFrame()\n","\n","tweetz = []\n","deltaz = []\n","datez  = []\n","for i in range(0, len(ML_df)):\n","\n","  temp_len = len(ML_df.iloc[i][\"Tweets no link\"])\n","  tweetz = tweetz+ list(ML_df.iloc[i][\"Tweets no link\"])\n","\n","  datez = datez + [ML_df.iloc[i][\"Date\"]] * temp_len\n","  if ML_df.iloc[i][\"Changed\"] == True:\n","    deltaz = deltaz + [1] * temp_len\n","  else:\n","    deltaz = deltaz + [0] * temp_len\n","\n","\n","res = pd.DataFrame([datez, tweetz, deltaz]).T\n","\n","res.columns = [\"Date\", \"Tweets\", \"Changed\"]\n","\n","res.to_csv(f\"{COMPLETED_DATA}MSFT_delta1_formated.csv\")"],"metadata":{"id":"edCyVcD-I-CL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f\"{COMPLETED_DATA}MSFT_delta1_formated.csv\"\n","\n","# res\n","# ML_df[\"Changed\"].iloc[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"V9MH2o8OKff9","executionInfo":{"status":"ok","timestamp":1677760713377,"user_tz":0,"elapsed":9,"user":{"displayName":"Bruce Lau","userId":"03714986190475957458"}},"outputId":"2c274c21-841e-49ce-ed0b-26b1c00de1ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/Text Mining Project IV Prediction/Data (Raw and processed data from python scripts)/Completed Data/MSFT_delta1_formated.csv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["**IMPORTANT**\n","- In the current data processing, numbers and links are excluded"],"metadata":{"id":"VvZfXOIaevRP"}},{"cell_type":"code","source":["# ============================== Data Cleansing ============================== #\n","# ============================================================================ #\n","\n","# Removing tweets with links:\n","ML_df[\"Tweets no link\"] = ML_df[\"Tweets\"].apply(\n","    lambda x: [i for i in x if \"http\" not in i])\n","\n","# Applying Ngrams:\n","n = 3\n","\n","# min number of of tweets\n","min_n = min(ML_df[\"Tweets\"].apply(len))\n","\n","ML_df[f\"Tweets {n}grams\"] = ML_df[\"Tweets no link\"].apply(\n","    lambda x: [ngrams(i, n) for i in x[:min_n]])\n","\n","\n","# Reshaping to 1d\n","ML_df[f\"Tweets {n}grams\"] = ML_df[f\"Tweets {n}grams\"].apply(all_in_one)\n","\n","# ============================ Vectorising Tweets ============================ #\n","# ============================================================================ #\n","\n","def vec(lizt):\n","  vec = vec = CountVectorizer()\n","  res = []\n","  for i in lizt:\n","    try:\n","      res.append(vec.fit_transform(i))\n","    except:\n","      continue\n","  return res\n","\n","\n","ML_df[f\"Tweets {n}grams\"] = ML_df[f\"Tweets {n}grams\"].apply(\n","    lambda x: vec(x[:min_n]))\n"],"metadata":{"id":"fxC6oT33pL9H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== Splitting Test and Training Data ===================== #\n","# ============================================================================ #\n","# Setting parameters\n","train_test_ratio = 0.75\n","train_quantity   = int(train_test_ratio*len(ML_df))\n","\n","\n","# Randomise order (Randomising order will ensure that iterative ML algos won't \n","# pick up wrong trends)\n","randomised_df = ML_df.sample(frac = 1)\n","\n","# Splitting train test data.\n","train = randomised_df.iloc[:train_quantity]\n","test  = randomised_df.iloc[train_quantity:]\n","\n","# ============================ Creating ML Model ============================= #\n","# ============================================================================ #\n","\n","# setting parameters\n","alpha = 0.1 # alpha value for elastic net ratio\n","            # Can experiment with different alpha to test ridge and lasso\n","\n","# creating lasso regression\n","lasso = Lasso(alpha = alpha)\n","\n","# creating elastic net regression\n","# elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n","\n","train_x, train_y = list(ML_df[f\"Tweets {n}grams\"].apply(lambda x: x[:200])), list(ML_df[\"Changed\"])\n","\n"],"metadata":{"id":"zwERjtOaUbzo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# np.array(train_y)\n","res = []\n","for i in train_y:\n","  if i:\n","    res.append(1)\n","  else:\n","    res.append(0)\n","\n","res_x = []\n","for i in train_x:\n","  res_x.append(np.array(i))\n","\n","\n","res_x = np.array(res_x)\n","\n","# lasso.fit(res_x, np.array(res))\n","\n","lasso.fit([[0,0], [1, 1], [2, 2]], [0, 1, 2])\n","\n","lasso.predict([[1,0]])\n","# res"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yHCXCKIj6Mxo","executionInfo":{"status":"ok","timestamp":1677686780348,"user_tz":0,"elapsed":655,"user":{"displayName":"Bruce Lau","userId":"03714986190475957458"}},"outputId":"d157d42d-7d2f-4c1d-8ba7-40624d643713"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1.])"]},"metadata":{},"execution_count":82}]},{"cell_type":"markdown","source":[],"metadata":{"id":"lb3Xv-zx31Ra"}},{"cell_type":"code","source":["from sklearn.linear_model import ElasticNet"],"metadata":{"id":"07sx9qE_71uA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def correct_HIV_date_format(string):\n","  date = string[-2:]\n","  month = string[-6:-3]\n","  new_date = string[:5]+date+month\n","\n","  \n","date = HIV_AAPL[\"Date\"].iloc[1][-2:]\n","month = HIV_AAPL[\"Date\"].iloc[1][-6:-3]\n","\n","new_date = HIV_AAPL[\"Date\"].iloc[1][:5]+date+month\n","new_date"],"metadata":{"id":"cPcHQEial4de"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MSFT = pd.read_csv(f\"{COMPLETED_DATA}MSFT.csv\")"],"metadata":{"id":"j39LtvlKnPhZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ML_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"6-iHUGvU6xDm","executionInfo":{"status":"ok","timestamp":1677688765189,"user_tz":0,"elapsed":1063,"user":{"displayName":"Bruce Lau","userId":"03714986190475957458"}},"outputId":"aff3aa84-87d6-4390-809c-1018d1a2d85f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          Date                                             Tweets  Changed  \\\n","0   2015-01-09  {How Sony's 'PlayStation Now' Can Shake Up The...    False   \n","1   2015-01-16  {As an $MSFT Research alum, I can tell you thi...    False   \n","2   2015-01-23  {$MSFT - 88% Barchart technical sell signals -...     True   \n","3   2015-01-30  {$MSFT A new multi-billion dollar market for M...     True   \n","4   2015-02-06  {Reports say @BillGates gave away another bill...     True   \n","..         ...                                                ...      ...   \n","255 2019-11-29  {$NTNX   Green and higher than body of ydays c...    False   \n","256 2019-12-06  {$MSFT Thanks for the quick fast , $CLWD Cloud...    False   \n","257 2019-12-13  {$AXSM  up 80%   Hope U don't want to be CUTE ...    False   \n","258 2019-12-20  {These last two up bars in $MSFT look promisin...    False   \n","259 2019-12-27  {@HalftimeReport @CNBC @GameStop @petenajarian...     True   \n","\n","                                        Tweets no link  \\\n","0    [Microsoft has just surpassed Exxon in market ...   \n","1    [$MSFT and $AAPL  charts side by side, almost ...   \n","2    [$MSFT - 88% Barchart technical sell signals -...   \n","3    [$MSFT just bought Sunrise, a very well design...   \n","4    [Reports say @BillGates gave away another bill...   \n","..                                                 ...   \n","255  [$NTNX   Green and higher than body of ydays c...   \n","256  [$MSFT Thanks for the quick fast , $MSFT wake ...   \n","257  [$AXSM  up 80%   Hope U don't want to be CUTE ...   \n","258  [These last two up bars in $MSFT look promisin...   \n","259  [@HalftimeReport @CNBC @GameStop @petenajarian...   \n","\n","                                         Tweets 3grams  \n","0    [  (0, 2)\\t1\\n  (1, 0)\\t1\\n  (2, 1)\\t1,   (0, ...  \n","1    [  (0, 2)\\t1\\n  (1, 1)\\t1\\n  (2, 0)\\t1,   (0, ...  \n","2    [  (0, 1)\\t1\\n  (2, 0)\\t1,   (1, 0)\\t1\\n  (2, ...  \n","3    [  (0, 2)\\t1\\n  (1, 1)\\t1\\n  (2, 0)\\t1,   (0, ...  \n","4    [  (0, 1)\\t1\\n  (1, 2)\\t1\\n  (2, 0)\\t1,   (0, ...  \n","..                                                 ...  \n","255  [  (0, 2)\\t1\\n  (1, 1)\\t1\\n  (2, 0)\\t1,   (0, ...  \n","256  [  (0, 1)\\t1\\n  (1, 2)\\t1\\n  (2, 0)\\t1,   (0, ...  \n","257  [  (0, 1)\\t1\\n  (1, 2)\\t1\\n  (2, 0)\\t1,   (0, ...  \n","258  [  (0, 1)\\t1\\n  (1, 0)\\t1\\n  (2, 2)\\t1,   (0, ...  \n","259  [  (0, 2)\\t1\\n  (1, 0)\\t1\\n  (2, 1)\\t1,   (0, ...  \n","\n","[260 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-ce3cb8e0-cd33-46df-8606-b0e107f5419e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Tweets</th>\n","      <th>Changed</th>\n","      <th>Tweets no link</th>\n","      <th>Tweets 3grams</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2015-01-09</td>\n","      <td>{How Sony's 'PlayStation Now' Can Shake Up The...</td>\n","      <td>False</td>\n","      <td>[Microsoft has just surpassed Exxon in market ...</td>\n","      <td>[  (0, 2)\\t1\\n  (1, 0)\\t1\\n  (2, 1)\\t1,   (0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2015-01-16</td>\n","      <td>{As an $MSFT Research alum, I can tell you thi...</td>\n","      <td>False</td>\n","      <td>[$MSFT and $AAPL  charts side by side, almost ...</td>\n","      <td>[  (0, 2)\\t1\\n  (1, 1)\\t1\\n  (2, 0)\\t1,   (0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2015-01-23</td>\n","      <td>{$MSFT - 88% Barchart technical sell signals -...</td>\n","      <td>True</td>\n","      <td>[$MSFT - 88% Barchart technical sell signals -...</td>\n","      <td>[  (0, 1)\\t1\\n  (2, 0)\\t1,   (1, 0)\\t1\\n  (2, ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2015-01-30</td>\n","      <td>{$MSFT A new multi-billion dollar market for M...</td>\n","      <td>True</td>\n","      <td>[$MSFT just bought Sunrise, a very well design...</td>\n","      <td>[  (0, 2)\\t1\\n  (1, 1)\\t1\\n  (2, 0)\\t1,   (0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2015-02-06</td>\n","      <td>{Reports say @BillGates gave away another bill...</td>\n","      <td>True</td>\n","      <td>[Reports say @BillGates gave away another bill...</td>\n","      <td>[  (0, 1)\\t1\\n  (1, 2)\\t1\\n  (2, 0)\\t1,   (0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>255</th>\n","      <td>2019-11-29</td>\n","      <td>{$NTNX   Green and higher than body of ydays c...</td>\n","      <td>False</td>\n","      <td>[$NTNX   Green and higher than body of ydays c...</td>\n","      <td>[  (0, 2)\\t1\\n  (1, 1)\\t1\\n  (2, 0)\\t1,   (0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>256</th>\n","      <td>2019-12-06</td>\n","      <td>{$MSFT Thanks for the quick fast , $CLWD Cloud...</td>\n","      <td>False</td>\n","      <td>[$MSFT Thanks for the quick fast , $MSFT wake ...</td>\n","      <td>[  (0, 1)\\t1\\n  (1, 2)\\t1\\n  (2, 0)\\t1,   (0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>257</th>\n","      <td>2019-12-13</td>\n","      <td>{$AXSM  up 80%   Hope U don't want to be CUTE ...</td>\n","      <td>False</td>\n","      <td>[$AXSM  up 80%   Hope U don't want to be CUTE ...</td>\n","      <td>[  (0, 1)\\t1\\n  (1, 2)\\t1\\n  (2, 0)\\t1,   (0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>258</th>\n","      <td>2019-12-20</td>\n","      <td>{These last two up bars in $MSFT look promisin...</td>\n","      <td>False</td>\n","      <td>[These last two up bars in $MSFT look promisin...</td>\n","      <td>[  (0, 1)\\t1\\n  (1, 0)\\t1\\n  (2, 2)\\t1,   (0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>259</th>\n","      <td>2019-12-27</td>\n","      <td>{@HalftimeReport @CNBC @GameStop @petenajarian...</td>\n","      <td>True</td>\n","      <td>[@HalftimeReport @CNBC @GameStop @petenajarian...</td>\n","      <td>[  (0, 2)\\t1\\n  (1, 0)\\t1\\n  (2, 1)\\t1,   (0, ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>260 rows Ã— 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ce3cb8e0-cd33-46df-8606-b0e107f5419e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ce3cb8e0-cd33-46df-8606-b0e107f5419e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ce3cb8e0-cd33-46df-8606-b0e107f5419e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":83}]},{"cell_type":"code","source":["ML_df.tweets()"],"metadata":{"id":"Wfo0IOPxpQRP"},"execution_count":null,"outputs":[]}]}