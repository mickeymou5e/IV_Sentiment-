{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1LGX8wi1E5wFCyvJ_r_ppksVX-0PnkRNI","authorship_tag":"ABX9TyNdngjD0Avt2HfMXQabn8vi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":12,"metadata":{"id":"dT9PPyS5yXnK","executionInfo":{"status":"ok","timestamp":1677178452525,"user_tz":0,"elapsed":15058,"user":{"displayName":"Bruce Lau","userId":"03714986190475957458"}}},"outputs":[],"source":["# Remember to \"mount drive\" first :\n","# Click on the folder icon on the left, and the third icon from the left is the \n","# mount drive function.\n","\n","import pandas as pd\n","import datetime as dt\n","\n","# Importing datasets\n","PATH_DATA      = \"/content/drive/MyDrive/Text Mining Project IV Prediction/Data/\"\n","RAW_DATA       = PATH_DATA + \"Raw Data/\"\n","PROCESSED_DATA = PATH_DATA + \"Processed Data/\"\n","\n","companies     = pd.read_csv(f\"{RAW_DATA}Top company tweets/Company.csv\")\n","company_tweet = pd.read_csv(f\"{RAW_DATA}Top company tweets/Company_Tweet.csv\")\n","tweets        = pd.read_csv(f\"{RAW_DATA}Top company tweets/Tweet.csv\")"]},{"cell_type":"code","source":["# List of companies\n","company_ticker = list(companies[\"ticker_symbol\"])\n","\n","# Creating dictionary of sets for each company's tweet id\n","tweet_id = company_tweet.groupby(\"ticker_symbol\")[\"tweet_id\"].apply(set)\n","tweet_id = tweet_id.to_dict()"],"metadata":{"id":"u_C2wptIzThe","executionInfo":{"status":"ok","timestamp":1677178464264,"user_tz":0,"elapsed":5266,"user":{"displayName":"Bruce Lau","userId":"03714986190475957458"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Converting date in the format of epoch to regular format\n","tweets[\"post_date\"] = [dt.datetime.fromtimestamp(epoch) for epoch in tweets[\"post_date\"]]"],"metadata":{"id":"MV9YowmymyR4","executionInfo":{"status":"ok","timestamp":1677178474661,"user_tz":0,"elapsed":8872,"user":{"displayName":"Bruce Lau","userId":"03714986190475957458"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Running through the \"tweets\" dataframe to find find tweets for each respective\n","# company, then exporting it\n","company_tweet_df = {}\n","\n","for company in company_ticker:\n","  print(f\"Searching for company {company}\")\n","  temp_rows = tweets[\"tweet_id\"].apply(lambda x: x in tweet_id[company])\n","  print(f\"Exporting {company}_tweets.csv\")\n","  tweets[temp_rows].to_csv(f\"{PROCESSED_DATA}{company}_tweets.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-VCmOTAQ0q2E","executionInfo":{"status":"ok","timestamp":1677178640594,"user_tz":0,"elapsed":60951,"user":{"displayName":"Bruce Lau","userId":"03714986190475957458"}},"outputId":"ead5a9dd-017d-4185-b71b-724ab1f634d9"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Searching for company AAPL\n","Exporting AAPL_tweets.csv\n","Searching for company GOOG\n","Exporting GOOG_tweets.csv\n","Searching for company GOOGL\n","Exporting GOOGL_tweets.csv\n","Searching for company AMZN\n","Exporting AMZN_tweets.csv\n","Searching for company TSLA\n","Exporting TSLA_tweets.csv\n","Searching for company MSFT\n","Exporting MSFT_tweets.csv\n"]}]}]}